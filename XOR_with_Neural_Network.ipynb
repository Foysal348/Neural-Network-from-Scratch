{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8B0Gc6Tx0A2TnVSFMivfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Foysal348/Neural-Network-from-Scratch/blob/main/XOR_with_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XOR with Neural Network using Sigmoid Function"
      ],
      "metadata": {
        "id": "pIxF45qC5m1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_l_H3noGPI6",
        "outputId": "48105cd8-4d7e-41e0-e3ff-1d29a146f17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output : [[0.20369158]\n",
            " [0.73603066]\n",
            " [0.73604444]\n",
            " [0.34370702]]\n",
            "Final error : [[-0.20369158]\n",
            " [ 0.26396934]\n",
            " [ 0.26395556]\n",
            " [-0.34370702]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Sigmoid Function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#Derivative of Sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "  return x*(1-x) #q(x)(1-q(x))\n",
        "\n",
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])#Input\n",
        "y=np.array([[0],[1],[1],[0]])#labeled/target\n",
        "\n",
        "np.random.seed(42)\n",
        "weights1=np.random.rand(2,2)# input → hidden\n",
        "weights2=np.random.rand(2,1) # hidden → output\n",
        "\n",
        "lr = 0.1  # Learning Rate\n",
        "#Loop\n",
        "for epoch in range(10000):\n",
        "  #Forward\n",
        "  layer1=sigmoid(np.dot(X,weights1))\n",
        "  output=sigmoid(np.dot(layer1,weights2))\n",
        "\n",
        "  #Backpropagation\n",
        "  error=y-output\n",
        "  d_output=error*sigmoid_derivative(output)#The gradient error of output layer\n",
        "  d_layer1=np.dot(d_output,weights2.T)*sigmoid_derivative(layer1)#The gradient error of hidden layer\n",
        "\n",
        "  #Update Weights\n",
        "  weights2 +=lr * np.dot(layer1.T,d_output)#weights = weights + lr × gradient\n",
        "  weights1 +=lr * np.dot(X.T,d_layer1)\n",
        "\n",
        "print(\"Final Output :\",output)\n",
        "print(\"Final error :\",error)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XOR with Neural Network using ReLU (hidden) + Sigmoid (output)"
      ],
      "metadata": {
        "id": "hgzocEEZ7ndZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Sigmoid Function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#Derivative of Sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "  return x*(1-x) #q(x)(1-q(x))\n",
        "\n",
        "#ReLU Function(hidden Layer)\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "#Derivative ReLU\n",
        "def relu_derivative(x):\n",
        "  return (x > 0).astype(float)# if x>0 ,then gradient=1,otherwise 0\n",
        "\n",
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])#Input\n",
        "y=np.array([[0],[1],[1],[0]])#labeled/target\n",
        "\n",
        "np.random.seed(42)\n",
        "weights1=np.random.rand(2,2)# input → hidden\n",
        "weights2=np.random.rand(2,1) # hidden → output\n",
        "\n",
        "lr = 0.1  # Learning Rate\n",
        "#Loop\n",
        "for epoch in range(10000):\n",
        "  #Forward\n",
        "  z1=np.dot(X,weights1)# hidden layer pre-activation\n",
        "  layer1=relu(z1)# hidden layer activation (ReLU)\n",
        "  z2=np.dot(layer1,weights2)# output layer pre-activation\n",
        "  output=sigmoid(z2)# output layer activation (Sigmoid)\n",
        "\n",
        "  #------------Backpropagation------------\n",
        "  error=y-output\n",
        "  # Output layer gradient\n",
        "  d_output=error*sigmoid_derivative(output)#The gradient error of output layer\n",
        "  # Hidden layer gradient\n",
        "  d_layer1=np.dot(d_output,weights2.T)*relu_derivative(z1)#The gradient error of hidden layer\n",
        "\n",
        "  #Update Weights\n",
        "  weights2 +=lr * np.dot(layer1.T,d_output)#weights = weights + lr × gradient\n",
        "  weights1 +=lr * np.dot(X.T,d_layer1)\n",
        "\n",
        "print(\"Final Output :\",output)#Stuck in training local minimum/plateau\n",
        "print(\"Final error :\",error)"
      ],
      "metadata": {
        "id": "VxZO8FcSHQV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b715e12-0bfd-4d9a-9d91-50eef4242d66"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output : [[0.5       ]\n",
            " [0.5000012 ]\n",
            " [0.49999884]\n",
            " [0.50000003]]\n",
            "Final error : [[-0.5       ]\n",
            " [ 0.4999988 ]\n",
            " [ 0.50000116]\n",
            " [-0.50000003]]\n"
          ]
        }
      ]
    }
  ]
}